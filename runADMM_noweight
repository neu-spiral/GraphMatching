#! /bin/bash
#SBATCH --job-name=FW
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=2Gb
#SBATCH --output=slurm-logs/FW.%j.out
#SBATCH --error=slurm-logs/FW.%j.err


module load spark/2.3.2-hadoop2.7
module load python/2.7.15



source /scratch/armin_m/spark/conf/spark-env.sh



ulimit -u 10000

G1=$1
G2=$2
N=$3
cnstMode=all
lamb=$4
attMode=M2


echo data/$G1/Dist_$G2$attMode 
python   LocalSolvers.py   data/$G1/objectives_$G2"_"$cnstMode  data/$G1/$G2"_"$cnstMode  data/ADMM/$G1$G2$cnstMode$attMode$lamb$p  --N $N  --dist data/$G1/Dist_$G2$attMode  --maxiters 10000   --lamb $lamb 

python  heatMap.py data/ADMM/$G1$G2$cnstMode$attMode$lamb$p"_P" data/$G1/graph data/$G2/graph  --outfile figs/HM/$G1$G2$cnstMode$attMode$lamb$p


