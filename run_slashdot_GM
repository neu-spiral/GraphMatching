#! /bin/bash
export BUCKET=armin-bucket
export GRAPH=slashdot
export homeDIR=/home/armin_mrm93/
export iter=1000
export config="spark.executor.memory=80g,spark.driver.memory=50g,spark.executor.instances=8,spark.executor.cores=32,spark.rpc.message.maxSize=1024,spark.driver.maxResultSize=20G"
export continue=0
export dump_freq=100
for N in 64;
do
    if [ $continue -eq 1 ]
    then
        gcloud dataproc jobs submit pyspark GraphMatching.py --properties $config  --cluster=foo  --py-files "helpers.py,LocalSolvers.py,debug.py" -- gs://$BUCKET/data/$GRAPH"/G"  $homeDIR$GRAPH"_"$N"_out" gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"  --graph1 gs://$BUCKET/data/$GRAPH"/graph" --graph2  gs://$BUCKET/data/$GRAPH"/graph"  --maxiter $iter  --N $N  --checkpointdir gs://armin-bucket/Checkpointing/$GRAPH"_"$N"parts_"  --logfile /home/armin_mrm93/$GRAPH"_"$N".log"   --checkpoint_freq 1000 --dump_trace_freq 20    --init gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"
    else
        gcloud dataproc jobs submit pyspark GraphMatching_2.py   --cluster=foo --py-files "helpers.py,LocalSolvers.py,debug.py" --properties "spark.executor.memory=80g,spark.executor.cores=32,spark.network.timeout=500" -- gs://$BUCKET/data/slashdot/G_WL5   $homeDIR$GRAPH"_"$N"_out" gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"  --objectivefile gs://$BUCKET/data/slashdot/objectives_WL5   --maxiter $iter  --N 5000  --checkpointdir gs://armin-bucket/Checkpointing/$GRAPH"_"$N"parts_"  --logfile /home/armin_mrm93/$GRAPH"_"$N".log"   --checkpoint_freq 10000 --dump_trace_freq 100 --rhoP 10.0 --rhoQ 5.0 --rhoT 5.0
    fi
done 
