
#! /bin/bash
export BUCKET=armin-bucket
export GRAPH=slashdot
export homeDIR=/home/armin_mrm93/
export iter=1000
export config="spark.executor.memory=100G,spark.driver.memory=20g,spark.rpc.message.maxSize=1024,spark.driver.maxResultSize=20G"         
export continue=0
export dump_freq=100
for N in 64;
do
    if [ $continue -eq 1 ]
    then
        gcloud dataproc jobs submit pyspark GraphMatching.py --properties $config  --cluster=foo  --py-files "helpers.py,LocalSolvers.py,debug.py" -- gs://$BUCKET/data/$GRAPH"/G"  $homeDIR$GRAPH"_"$N"_out" gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"  --graph1 gs://$BUCKET/data/$GRAPH"/graph" --graph2  gs://$BUCKET/data/$GRAPH"/graph"  --maxiter $iter  --N $N  --checkpointdir gs://armin-bucket/Checkpointing/$GRAPH"_"$N"parts_"  --logfile /home/armin_mrm93/$GRAPH"_"$N".log"   --checkpoint_freq 1000 --dump_trace_freq 20    --init gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"
    else
        gcloud dataproc jobs submit pyspark GraphMatching.py --properties $config  --cluster=foo --py-files "helpers.py,LocalSolvers.py,debug.py" -- gs://$BUCKET/data/slashdot/G_WL5   $homeDIR$GRAPH"_"$N"_out" gs://$BUCKET/$GRAPH"_"$N"_out_"$iter"iterations"  --objectivefile gs://$BUCKET/data/slashdot/objectives_WL5   --maxiter $iter  --N 20000  --checkpointdir gs://armin-bucket/Checkpointing/$GRAPH"_"$N"parts_"  --logfile /home/armin_mrm93/$GRAPH"_"$N".log"   --checkpoint_freq 10000 --dump_trace_freq 100 --rhoP 10.0 --rhoQ 5.0 --rhoT 5.0
    fi
done 
