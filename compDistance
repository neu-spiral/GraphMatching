#! /bin/bash
#SBATCH --job-name=distance
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1Gb
#SBATCH --output=slurm-logs/preproc.%j.out
#SBATCH --error=slurm-logs/preproc.%j.err


module load spark/2.3.2-hadoop2.7
module load python/2.7.15



source /scratch/armin_m/spark/conf/spark-env.sh



ulimit -u 10000


spark-submit --master local[40]   --py-files "helpers.py"  --executor-memory 100g --driver-memory 100g   Distance.py   $1  $2   $3   --connection_graph $4 --equalize --N $5 



